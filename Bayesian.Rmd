--- 
title: <span style="font-size:150%; font-variant:small-caps; font-style:italic; ">Become a Bayesian with R & Stan</span>
author:  |
  <div class="title"><span style="font-size:125%; font-variant:small-caps; font-style:normal">Michael Clark</span><br>
  <span style="font-size:75%; margin: 0 auto; font-style:normal">Statistician Lead</span> <br>
  <img src="../img/CSCAR_logos/signature-acronym.png" style="width:24%; padding:10px 0;"> <br>
  <img src="../img/ARC_logos/ARC-acronym-signature.png" style="width:17%; padding:10px 0;"> </div>
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
    bookdown::tufte_html_book: 
      toc: yes
      number_sections: false
      css: [toc_test.css, notebooks.css]
      split_by: rmd
    # tint::tintHtml:
    #   default: TRUE
    #   self_contained: TRUE
    # bookdown::gitbook:
    #   css: [notebooks.css]
    #   highlight: pygments
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: m-clark/Workshops
description: "An introduction to using R for Bayesian data analysis."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache=F, message = F, warning=F, 
                      R.options=list(width=120), fig.align='center')

# automatically create a bib database for R packages
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'), 'packages.bib')
```

```{r echo=FALSE}
library(tufte); library(tidyverse)
```


#  <span style="color:transparent">Home</span>

<!--chapter:end:index.Rmd-->

# Introduction
<!-- `r newthought('Statistical modeling is a thoughtful exercise.')` will not create pdf on work machine but html ok despite errors-->
`r newthought('Statistical modeling is a thoughtful exercise.')` Bayesian approaches allow for us to put even more thought into the standard modeling approach, to explore our models more deeply, and may enable a better understanding of the uncertainty therein.  This document (and related talk[^talk]) has a primary purpose of providing an introduction to tools within R that can be used for Bayesian data analysis, and an introduction to the Stan programming language they depend on.  It is not an introduction to Bayesian inference (see the [references section][References] and [my intro](http://m-clark.github.io/docs/IntroBayes.html)), nor an introduction to statistical modeling in R.  However, some introductory concepts will be presented for those who may be new to Bayesian data analysis.

A few points that serve to illustrate the perspective taken here:

- In this day and age, there is no more need to justify the use of a Bayesian approach than there is a traditional one.  In fact, if one is using the standard null hypothesis testing approach, they may need to justify that much more so.  The Bayesian approach may be new to you, but its concepts are very old, and the techniques are widely used across many disciplines.

- Proselytizing is not a goal here, the only zealotry is perhaps for R and Stan as useful tools.  Bayesian approaches are still too cumbersome in some settings (e.g. with very large data), and I have no problem using more pragmatic tools.  

- Details will be glossed over. If one has prior exposure to Bayesian data analysis, this can simply be used to learn a couple tools quickly. For those new to the Bayesian approach, you will only get a glimpse of what is to come.


## Outline 


This document will provide a basic introduction to the probabilistic programming language Stan, specifically focusing on its usage via R.  A very brief overview of the Bayesian modeling approach will be provided as a starting point, followed by a description of the Stan language and the constituent parts of a Stan model.  Three package implementations available in R will then be demonstrated- <span class="pack">rstan</span>, <span class="pack">rstanarm</span>, and <span class="pack">brms</span>.


- Become a Bayesian in 10 minutes
- Key concepts
- The Stan Modeling Language
- R package demonstrations
- Model Extension demonstrations



## Prerequisites {.unnumbered} 

As far as statistical modeling goes, no more than a standard exposure to regression is assumed.  You would do a lot better knowing the basics of maximum likelihood estimation as well.  As for R, you should be able to run basic lm/glm models in that environment.


Color coding:

- <span class="emph">emphasis</span>
- <span class="pack">package</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>
- [link][Prerequisites]








[^talk]: The associated talk was not a hands-on workshop.

<!--chapter:end:01_intro.Rmd-->

# Key Concepts

This section focuses on some key ideas to help you quickly get into the Bayesian mindset.  Again, very little statistical knowledge is assumed.  If you have prior experience with the Bayesian approach it may be skipped.

<img src="img/priorLikePosterior_test1.png" style="display:block; margin: 0 auto;">

## Distributions

One of the first things to get used to coming from a traditional framework is that the focus is on distributions of parameters rather than single estimates of them.  In the Bayesian context, parameters are <span class="emph">*random*</span>, not fixed. Your analysis will start with one distribution, the <span class="emph">prior</span>, and end with another, the <span class="emph">posterior</span>.  The summaries of that distribution, e.g. the mean, standard deviation, etc. will be available, and thus be used to understand the parameters in similar fashion as the traditional approach.

As an example, if you want to estimate a regression coefficient, the Bayesian analysis will result in hundreds to thousands of values from the distribution for that coefficient.  You can then use those values to obtain their mean, or use the quantiles to provide an interval estimate, and thus end up with the same type of information.

Consider the following example.  We obtain a 1000 draws from a normal distribution with mean 5 and standard deviation of 2.  From those values we can get the mean or an interval estimate.

```{r distribution}
coef_result = rnorm(1000, 5, 2)
head(coef_result)
mean(coef_result)
sd(coef_result)
quantile(coef_result, c(.025,.975))
```

You will end up specifying the nature of that distribution depending on the model and goals of your situation, but the concept will be no different.


## Prior

For the Bayesian approach we must choose a <span class="emph">prior distribution</span> representing our initial beliefs about the estimate.  This is traditionally where some specifically have difficulty with Bayesian estimation, and newcomers are most wary.  You will have to make choices here, but they are no different than the sort of choices you've always made in statistical modeling.  Perhaps you let the program do it for you (generally a bad idea), or put little thought into it (also a bad idea), but such choices were always there.  Examples include which variables go into the model, the nature of the likelihood (e.g. normal vs. Poisson), whether to include interactions, etc.  You're *always* making these choices in statistical modeling. *Always*. If it didn't bother you before it needn't now.

The prior's settings are typically based on modeling concerns.  As an example, the prior for regression coefficients could be set to uniform with some common sense boundaries. This would more or less defeat the purpose of the Bayesian approach, but it represents a situation in which we are completely ignorant of the situation. In fact, doing so would produce the results from standard likelihood regression approaches, and thus you can think of your familiar approach as a Bayesian one with uninformed priors.  However, more common practice usually sets the prior for a regression coefficient to be normal, with mean at zero and with a relatively large variance.  The large variance reflects our ignorance, but using the normal results in nicer estimation properties.  The really nice thing is however, that we could have set the mean to that seen for the same or similar situations in prior research.  In this case we can build upon the work that came before.

Setting aside the fact that such 'subjectivity' is an inherent part of the scientific process, and that ignoring prior information, if explicitly available from previous research, would be blatantly unscientific, the main point to make here is that this choice is not an *arbitrary* one. There are many distributions we might work with, but some will be better for us than others. And we can always test different priors to see how they might affect results (if at all)[^sens].

## Likelihood

I won't say too much about the <span class="emph">likelihood function</span> here. I have a refresher in the appendix of the [Bayesian Basics doc](http://m-clark.github.io/docs/IntroBayes.html). In any case here is a brief example.  We'll create a likelihood function for a standard regression setting, and compare results for two estimation situations.

```{r like}
# likelihood function
reg_ll = function(X, y, beta, sigma){
  sum(dnorm(y, mean=X%*%beta, sd=sigma, log=T))
}

# true values
true_beta = c(2,5)
true_sigma = 1

# comparison values
other_beta = c(0,3)
other_sigma = 2

# sample size
N = 1000

# data generation
X = cbind(1, runif(N))
y = X %*% true_beta + rnorm(N, sd=true_sigma)

# calculate likelihooods
reg_ll(X, y, beta=true_beta, sigma=true_sigma)    # more likely
reg_ll(X, y, beta=other_beta, sigma=other_sigma)  # less likely
logLik(lm(y~., data=data.frame(X[,-1])))          # actual log likelihood
```

The above demonstrates a couple things.  The likelihood tells us the relative number of ways the data could occur given the parameter estimates. For a standard linear model where the likelihood is based on a normal distribution, we require estimates for the coefficients and the variance/standard deviation. In the standard maximum likelihood (ML) approach commonly used in statistical analysis, we use an iterative process to end up with estimates of the parameters that maximize the data likelihood.  With more data, and a lot of other considerations going in our favor, we end up closer to the true values[^trueparm].  A key difference from standard ML methods and the Bayesian approach is that the former assumes a fixed parameter, while the Bayesian approach assumes the parameter is *random*.

## Posterior

The <span class="emph">posterior</span> distribution is a weighted combination of the prior and the likelihood, and is proportional to their product. Assuming some $\theta$ is the parameter of interest:

$$ p(\theta|Data) \propto p(Data|\theta) \cdot p(\theta) $$
$$ \;\;\mathrm{posterior} \;\propto \mathrm{likelihood} \;\!\cdot \mathrm{prior} $$
With more data, i.e. evidence, the weight shifts ever more to the likelihood, ultimately rendering the prior inconsequential. Let's now [see this in action](http://micl.shinyapps.io/prior2post/).


## P-values

One of the many nice things about the Bayesian approach regards the probabilities and intervals we obtain, specifically their interpretation.  For some of you still new to statistical analysis in general, this may be the interpretation you were already using, though incorrectly.

As an example, the p-value from standard <span class="emph">null hypothesis testing</span> goes something like the following:

> *If the null hypothesis is true*, the probability of seeing a result like this or more extreme is P.

Contrast this with the following:

> The probability of this result being different from zero is P.

Which is more straightforward? Now consider the interval estimates:

> <p class="fragment"> If I repeat this study precisely an infinite number of
times, and I calculate a P% interval each time, then P% of those  intervals will
contain the true parameter.</p>

Or:

> <p class="fragment">P is the probability the parameter falls in *this* interval.</p>

One of these reflects the notions and language we use in everyday speech, the other hasn't been understood very well by most people practicing science.  While oftentimes, and especially for simpler models, a Bayesian interval will not be much different than the traditional one, at least it will be something you could describe to the average Joe on the street.  In addition, you can generate distributions, and thus estimates with corresponding intervals, for anything you can calculate based on the model.  Such statistics are a natural by-product of the approach, and make it easier to explore your data further.


[^sens]: Such testing is referred to as sensitivity analysis.
[^trueparm]: Assuming such a thing exists.

<!--chapter:end:02_key_concepts.Rmd-->

# Stan

<img src="../img/stan_logo.png" style="display:block; margin: 0 auto; width:25%">

Stan is a modeling language for Bayesian data analysis[^stan1].  The actual work is done in C++, but the Stan language specifies the necessary aspects of the model. It uses a variety of inference procedures, including standard optimization techniques commonly found elsewhere, but the primary Bayesian-specific approach regards Hamiltonian Monte Carlo[^HMC].  There are actually a number of ways in which Bayesian estimation/sampling can take place and this is one that has, like the others, advantages in some areas, and disadvantages in others[^HMC2]. It is however applicable in a wide variety of problems and will often do better than other approaches.

To use Stan directly, one just needs to know their model intimately, which should be the case if you're going to spend so much time in collecting, processing and analyzing data in the first place.  I personally find the *style* of the Stan language clear, and it might be seen as a combination of C++ and R programming styles, though mostly the latter.  The following will take you through the components of the Stan language. 

## Installing Stan

To get started with Stan (in R), one needs to install the <span class="pack">rstan</span> package. While this will proceed as with any other package, additional steps are required.  First, you will need a C++ compiler, and this process will be different depending on your operating system.  It won't take much, there's a chance you already have one even, but steps are clearly defined on the [RStan wiki](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started).  After going through that process, you may then install the <span class="pack">rstan</span> package and dependencies.  You'll be ready to go at that point.

## The Way of Stan/RStan

The basic workflow you'll engage in to run a Stan program within R is as follows:

- Write the Stan program
- Create a data list
- Run a debug model to check compilation etc.
- Run the full model
- Summarize the model
- Check diagnostics, including posterior predictive inspection

In this part we'll consider the elements of the Stan program.


## Elements of a Stan Program

The following shows the primary parts of a Stan program for a standard linear model.  We'll go through each component in turn.

```{stan stanInit, output.var="stanmodel", eval=F}
data {                      // Data block
  int<lower=1> N;           // Sample size
  int<lower=1> K;           // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}

transformed data {          // Transformed data block.
} 

parameters {                // Parameters block
  vector[K] beta;           // Coefficient vector
  real<lower=0> sigma;      // Error scale
}

transformed parameters {    // Transformed parameters block.
} 

model {                     // Model block
  vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  
  // priors
  beta ~ normal(0, 10);
  sigma ~ cauchy(0, 5);     
  
  // likelihood
  y ~ normal(mu, sigma);
}

generated quantities {      // Generated quantities block. 
}
```

For a reference, the following is from the Stan manual, variables of interest and the associated blocks where they would be declared: 

```{r stanBlocks, results='asis', echo=FALSE}
tab = data.frame(c('modeled, unmodeled data', 'modeled parameters, missing data', 'unmodeled parameters', 'generated quantities', 'loop indices'),
                 c('data, transformed data', 'parameters, transformed parameters', 'data, transformed data', 'transformed data, transformed parameters, generated quantities','loop statement'))
colnames(tab) = c('Variable Kind', 'Declaration Block')
htmlTable::htmlTable(tab, rnames=F)
```


### Data

```{stan stanData, output.var="stanmodel", eval=F}
data {                      // Data block
  int<lower=1> N;           // Sample size
  int<lower=1> K;           // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}
```

The first section is the <span class="emph">data</span> block, where we tell Stan the data it should be expecting from the data list.  It is useful to put in bounds as a check on the data input, and that is what is being done between the < > (e.g. we should at least have a sample size of 1).  The first two variables declared are N and K, both as integers.  Next the code declares the model matrix and target vector respectively. As you'll note here and for the next blocks, we declare the type and dimensions of the variable and then its name. In Stan, everything declared in one block is available to subsequent blocks, but those declared in a block may not be used in earlier blocks. Even within a block, anything declared, such as N and K, can then be used subsequently, as we did to specify dimensions.


### Transformed Data

```{stan stanTdata, output.var="shouldntbenecessary", eval=F}
transformed data {          // Transformed data block
  vector[N] logX;
  
  logX = log(X);
} 
```

The <span class="emph">transformed data</span> block is where you could do such things as log or center variables and similar, i.e. you can create new data based on the input data or just in general.  If you are using R though, it would almost always be easier to do those things in R first and just include them in the data list.  You can also declare any unmodeled parameters here.


### Parameters

```{stan stanParams, output.var="shouldntbenecessary", eval=F}
parameters {                // Parameters block
  vector[K] beta;           // Coefficient vector
  real<lower=0> sigma;      // Error scale
}
```


The primary parameters of interest that are to be estimated go in the <span class="emph">parameters </span> block.  As with the data block you can only declare these variables, you cannot make any assignments.  Here we note the $\beta$ and $\sigma$ to be estimated, with a lower bound of zero on the latter. In practice you might prefer to split out the intercept or other coefficients to be modeled separately if they are on notably different scales.


### Transformed Parameters

```{stan stanTParams, output.var="shouldntbenecessary", eval=F}
transformed parameters {    // Transformed parameters block
  real newpar;
  
  newpar = exp(oldpar);
} 
```



The <span class="emph">transformed parameters</span> block is where optional parameters of interest might be included.  What might go here is fairly open, but for efficiency's sake you will typically want to put things only of specific interest that are dependent on the parameters block.  These are evaluated along with the parameters, so if they are not of special interest you can generate them in the model or generated quantities block to save time and space.


### Model
```{stan stanModel, output.var="shouldntbenecessary", eval=F}
model {                     // Model block
  vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  
  // priors
  beta ~ normal(0, 10);
  sigma ~ cauchy(0, 5);     
  
  // likelihood
  y ~ normal(mu, sigma);
}

```


The <span class="emph">model</span> block is where your priors and likelihood are specified, along with the declaration of any variables necessary.  As an example, the linear predictor is included here, as it will go towards the likelihood`r margin_note("The position within the model block isn't crucial. I tend to like to do all the variable declarations at the start, but others might prefer to have them under the likelihood heading at the point they are actually used.")`.  Note that we could have instead put the linear predictor in the transformed parameters section, but this would slow down the process, and again, we're not so interested in those specific values. 


### Generated Quantities
```{stan stanGenQuan, output.var="shouldntbenecessary", eval=F}
generated quantities {
  vector[N] yhat;                // linear predictor
  real<lower=0> rss;             // residual sum of squares
  real<lower=0> totalss;         // total SS              
  real Rsq;                      // Rsq
  
  yhat = X * beta;
  rss = dot_self(y-yhat);
  totalss = dot_self(y-mean(y));
  Rsq = 1 - rss/totalss;
}
```

The <span class="emph">generated quantities</span> block is a fantastical place where anything in your noggin can spring forth to life. *Anything* that you can think of that can be calculated based on the model results can be assessed here.  What's more, it will have a distribution just like everything else.  The above calculates the typical R^2^ as an example.  Because of the priors, it is already 'adjusted', but we also have an interval estimate for it.  

As another example, to get a sense of how well you're capturing the tails of the distribution for `y`, you could start by calculating your minimum mean prediction, and see how often it falls below the true minimum.  From the Bayesian approach we could not only get an interval of the minimum prediction, we'd get the probability for how often it is above or below the true minimum, which is simply the proportion of samples for which this takes place.  Ideally we'd like a symmetric distribution for the estimated minimum around the true minimum with no general tendency to be above or below.  If it was a very high probability of being lower than the true minimum, perhaps the model over compensates for the lower tail of the distribution.  If it was very low, it would perhaps signal we are not capturing lower extremes very well.  We could do this for the maximum or any other value of interest.


## Using Stan

Now that you have a Stan program in place, you're ready to proceed.  



[^stan1]: Stan 1.0 was released in 2012.

[^HMC]: Originally called *Hybrid* Monte Carlo, such a name is a bit too vague for most.

[^HMC2]: For many types of problems, relative to Gibbs and some other samplers, HMC will be more efficient in the sense it will take fewer iterations to describe the posterior distribution.

<!--chapter:end:03_stan.Rmd-->

# R

R has many tools for Bayesian analysis, and possessed these before Stan came around.  Among the more prominent were those that allowed the use of BUGS (e.g. <span class="pack">r2OpenBugs</span>), one of its dialects JAGS (<span class="pack">rjags</span>), and packages like <span class="pack">coda</span> and <span class="pack">MCMCpack</span> that allowed for customized approaches, further extensions or easier implementation.  Other packages might regard a specific type or family of models (e.g. <span class="pack">bayesm</span>), but otherwise be mostly R-like in specifying the model (e.g. <span class="pack">MCMCglmm</span> for mixed models).  

Now it is as easy to conduct standard and more complex models using Stan while staying within the usual framework of R-style modeling.  You don't even have to write Stan code!  I'll later note a couple relevant packages that enable this.


## rstan

The <span class="pack">rstan</span> package is the workhorse, and the other packages mentioned in following rely on it or assume similarities to it.  In general though, <span class="pack">rstan</span> is what you will use when you write Stan code directly. The following demonstrates how.

### Data list

First you'll need to create a list of objects we'll call the <span class="emph">data list</span>. It is a list of *named* objects that Stan will look to match to the things you noted in the `data{}` section of your Stan code.  In our example, our data statement has four components- `N` `K` `X` and `y`.  As such, we might create the following data list.

```{r dataList, eval=FALSE}
dataList =  list(X=mymatrix, y=mytarget, N=1000, K=ncol(mymatrix))
```

You could add fixed parameters and similar if your Stan code relies on them somewhere, but at this point you're ready to proceed.  Here is a model using RStan.

```{r rstan, eval=FALSE}
library(rstan)

modResults = stan(mystancode, data=dataList, iter=2000, warmup=1000)
```

The Stan code is specified as noted previously, and can be a string in the R environment, or a separate text file[^stringvsfile].  

### Debug model

The debug model is just like any other except you'll only want a couple iterations and for one chain.

```{r rstanDebug, eval=FALSE}
model_debug = stan(mystancode, data=dataList, iter=10, chains=1)
```

This will allow you to make sure the Stan code  compiles first and foremost, and secondly, that there aren't errors in the program that keep parameters from being estimated (thus resulting in no posterior samples).  For a compile check, you hardly need any iterations. However, if you set the iterations a little higher, you may also discover potential difficulties in the estimation process that might suggest code issues remain.


### Full model

If all is well with the previous, you can now proceed with the main model.  Setting the argument `fit = debugModel` will save you the time spent compiling. It is a notable time saver to run the chains in parallel by setting `cores = ncore`, where `ncore` is some value representing the number of cores on your machine you want to use.

```{r rstanFullModel, eval=FALSE}
mystanmodel = stan(mystancode, data=dataList, fit = model_debug, 
                   iter=2000, warmup=1000, thin=1, chains=4, cores=4)
```

Once you are satisfied that the model runs well, you really only need one chain if you rerun it in the future.


### Model summary

The typical model summary provides parameter estimates, standard errors, interval estimates and two diagnostics- effective sample size, the $\hat{R}$ diagnostic.

```{r runStan, eval=FALSE, echo=FALSE}
stanmod = '
functions {
  vector stdized(int N, vector x, int scale) {
     vector[N] x_sc;

     x_sc = scale ? x-mean(x) : (x-mean(x))/sd(x);
     
     return x_sc;
  }
}

data {                      // Data block
  int<lower=1> N;           // Sample size
  int<lower=1> K;           // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}

transformed data {          // Transformed data block.
} 

parameters {                // Parameters block
  vector[K] beta;           // Coefficient vector
  real<lower=0> sigma;      // Error scale
}

transformed parameters {    // Transformed parameters block.
} 

model {                     // Model block
  vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  
  // priors
  beta ~ normal(0, 10);
  sigma ~ cauchy(0, 5);     
  
  // likelihood
  y ~ normal(mu, sigma);
}

generated quantities {         // Generated quantities block. 
  vector[N] yhat;              // predicted y
  real Rsq;                    // R-squared
  real Intercept;
  real beta_1;
  vector[N] test;
  
  yhat = X * beta;
  Rsq = 1 - dot_self(y-yhat)/dot_self(y-mean(y));
  
  Intercept = beta[1];
  beta_1 = beta[2];
  test  = stdized(N, y, 1);
}'
n = 500
X = cbind(1, runif(n))
y = c(X%*%c(1,.5)) + rnorm(n)
dataList = list(X=X, y= y + rnorm(n), N=nrow(X), K=ncol(X))
library(rstan)
mystanmodel = stan(model_code = stanmod, data=dataList, cores=4)
save(mystanmodel, dataList, file='data/stanmod.RData')
```

```{r, eval=T, echo=3}
load('data/stanmod.RData')
library(rstan)
print(mystanmodel, pars=c('Intercept', 'beta_1', 'sigma', 'Rsq'), probs = c(.05,.95), digits=3)
# broom::tidy(mystanmodel) %>% 
#   filter(term %in% c('Intercept', 'beta_1', 'sigma', 'Rsq')) %>% 
#   pander::pander()
```



### Diagnostics and beyond

Typical Bayesian diagnostic tools like trace plots, density plots etc. are available.  Part of the printed output contains the two just mentioned. In addition <span class="pack">rstan</span> comes with model comparison functions like <span class="func">WAIC</span> and <span class="func">loo</span>.  The best part is the <span class="func">launch_shiny</span> function, which actually makes this part of the analysis a lot more fun.  Below is a graphical depiction of what would open in your browser when you use the function.

```{r eval=FALSE}
library(shinystan)
launch_shiny(mystanmodel)
```

<img src="img/shinystan.png" style="display:block; margin: 0 auto;">

## rstanarm

The <span class="pack">rstanarm</span> is a package from the Stan developers that allows you to specify models in the standard R format`r margin_note("The 'arm' in rstanarm is for 'applied regression and multilevel modeling', which is *NOT* the title of Gelman's book no matter what he says.")`.  While this is very limiting, it definitely covers a lot of the usual statistical ground.  As such, it enables you to be a Bayesian for any of the very common glm settings, including mixed and additive models.

Key modeling functions include:

- <span class="func">stan_lm</span>: as with lm
- <span class="func">stan_glm</span>: as with glm 
- <span class="func">stan_glmer</span>: generalized linear mixed models
- <span class="func">stan_gamm4</span>: generalized additive mixed models
- <span class="func">stan_polr</span>: ordinal regression models

Other functions allow the ability to change priors, enable posterior predictive checking etc. The following shows example code.


```{r rstanarm, eval=FALSE, echo=3:6}
load('data/stanmod.RData')
mydataframe = data.frame(y = dataList$y, x = dataList$X[,2])
library(rstanarm)

rstanarm_results = stan_glm(y ~ x, data=mydataframe, iter=2000, warmup=1000, cores=4)
summary(rstanarm_results, probs=c(.025, .975), digits=3)
# save(mystanmodel, dataList, rstanarm_results, file='data/stanmod.RData')
```

```{r rstanarmDisplay, echo=F, R.options=list(width=120)}
load('data/stanmod.RData')
summary(rstanarm_results, probs=c(.025, .975), digits=3)
```


The resulting model object can essentially be used just like the <span class="func">lm</span>/<span class="func">glm</span> functions in base R.  There are <span class="func">summary</span>, <span class="func">predict</span>, <span class="func">fitted</span>, <span class="func">coef</span> etc. functions available to use just like with standard model objects.

## brms

I have watched with much enjoyment the development of the <span class="pack">brms</span> package from nearly its inception.  Due to the continued development of <span class="pack">rstanarm</span>, it's role is becoming more niche perhaps, but I still believe it to be both useful and powerful.  It allows for many types of models, custom Stan functions, and many distributions (including truncated versions, ordinal variables, zero-inflated, etc.).  The main developer is ridiculously responsive to requests, so extensions are regularly implemented.  In short, for standard models you can use <span class="pack">rstanarm</span>, while for variations of those, more flexible manipulation of priors, or more complex models, you can use <span class="pack">brms</span>.


The following shows an example of the additional capabilities provided by the <span class="func">brm</span> function, which unlike rstanarm, is the only function you need for modeling with this package.  The following demonstrates use of a truncated distribution, an additive model with random effect, use of a different family function, specification of prior distribution for the fixed effect coefficients, specification of correlated residual structure, optional estimation algorithm, and use of custom Stan functions.

```{r brms, eval=FALSE}
library(brms)

modResults = brm(y | trunc(lb = 0, ub = 100) ~ s(x) + (1|id), family=student, data=dataList, 
                 prior = set_prior('horseshoe(3)', class='b'),
                 autocor = cor_ar(~patient, p = 1),
                 algorithm = 'meanfield',
                 stan_funs = stdized,
                 iter=2000, warmup=1000)
```

The <span class="pack">brms</span> package also has a lot of the same functionality for post model inspection.

## rethinking

The <span class="pack">rethinking</span> package accompanies the text, Statistical Rethinking by Richard McElreath.  This is a great resource for learning Bayesian data analysis while using Stan under the hood. You can do more with the other packages mentioned, but if you can also run your model here, you might get even more to play with.  Like <span class="pack">rstanarm</span> and <span class="pack">brms</span>, you might be able to use it to produce starter Stan code as well, that you can then manipulate and use via <span class="pack">rstan</span>.  Again, this is a very useful tool to learn Bayesian analysis in general, especially if you have the text.



## Summary

To recap, we can summarize the roles of these packages as follows (ordered from easiest to more flexible):

- <span class="pack">rethinking</span>: Good resource to introduce yourself to Bayesian analysis.

- <span class="pack">rstanarm</span>: All you need to start on your path to using Bayesian techniques. Keeps you in more familiar modeling territory so you can focus on learning the new stuff. Supports up through mixed models, GAMs, and ordinal models.

- <span class="pack">brms</span>: Take your model notably further and still not have to write raw stan code.  Supports a very wide range of models and still without raw Stan code.

- <span class="pack">rstan</span>: Here you write your Stan code regarding whatever model your heart desires, then let rstan do the rest.

- raw R or other: Some still insist on writing their own sampler.  While great for learning purposes, this is mostly a good way to produce buggier code, have less model exploration capability, all while being a lot less efficient (and very slow if in raw R).  Maybe you'll end up here, but you should exhaust your other possibilities first.

[^stringvsfile]: I would maybe suggest using strings with simple models as you initially learn Stan/RStan, but a separate file is preferred. RStudio has syntax highlighting and other benefits for *.stan files.

[^trunc]: Or Bayesian modeling generally.

<!--chapter:end:04_R.Rmd-->

# Extensions

## R

Some R extensions for Stan include the following:

```{r whichRstan, eval=F, echo=FALSE}
packs = data.frame(installed.packages())
packs %>% filter(Package %in% c('rstan', 'rstanarm', 'brms', 'loo', 'shinystan'))
sapply(installed.packages()[,'Suggests'], strsplit, ',') %>% 
  sapply(function(x) x %in% 'rstan') %>% 
  sapply(any) %>% which
sapply(installed.packages()[,'Suggests'], strsplit, ',') %>% 
  sapply(grep, pattern='stan') %>% 
  sapply(any) %>% which
```


- <span class="pack">shiny_stan</span>: for interactive diagnostics

`r tufte::margin_note('Screen shot of shiny_stan')`
<img src="img/shinystan.png" style="display:block; margin: 0 auto; width:75%">

- <span class="pack">loo</span>: Provides approximate leave-one-out cross-validation statistics for model comparison.

- <span class="pack">bayesplot</span>: diagnostic plots and other useful tools, though notable overlap with shiny_stan


`r tufte::margin_note('Example plots from bayesplot')`
```{r bayesplotEx, eval=T, echo=FALSE, dev='svg', fig.align='center', fig.width=4, fig.height=3, cache=TRUE}
load('data/stanmod.RData')
library(ggplot2); library(bayesplot)
color_scheme_set("red")
ppc_dens_overlay(y = rstanarm_results$y, 
                 yrep = rstanarm::posterior_predict(rstanarm_results, draws = 50))
# plotly::ggplotly()
```


```{r nuts, eval=T, echo=FALSE, dev='svg', fig.align='center', fig.width=4, fig.height=3, cache=TRUE}
np <- nuts_params(rstanarm_results)
mcmc_nuts_energy(np, merge_chains = T) + ggtitle("NUTS Energy Diagnostic")
```


## Stan functions

One can write their own Stan functions just like with R.  Just start your model code with a `functions {}` block.  Perhaps you will need something to make later code a little more efficient, or a specific type of calculation.  You can create a custom function to suit your needs.  An example function below standardizes a variable to have a mean of 0 and standard deviation of 1, or just center it if scale=0.

```{stan stanFunc, output.var="stanmodel", eval=F}
functions {
  vector stdized(int N, vector x, int scale) {
     vector[N] x_sc;

     x_sc = scale ? x-mean(x) : (x-mean(x))/sd(x);
     
     return x_sc;
  }
}
```

Presumably this capability will result in custom modules that are essentially the equivalent of R packages for Stan.  However, at this time there doesn't look to be much in this regard.

## Other frameworks

Stan goes beyond R, so if you find yourself using other tools but still need the power of Stan, fret not.

- <span class="pack">CmdStan</span>: shell, command-line terminal
- <span class="pack">PyStan</span>: Python
- <span class="pack">StataStan</span>: Stata
- <span class="pack">MatlabStan</span>: MATLAB
- <span class="pack">Stan.jl</span>: Julia
- <span class="pack">MathematicaStan</span>: Mathematica

<!--chapter:end:05_extensions.Rmd-->


# Conclusion 


Hopefully this talk and set of notes has given a quick overview of how easy it is to get started with Bayesian analysis using Stan and R.  If you are just starting out with Bayesian analysis, you now have several resources at your disposal to jump right in, so you can spend more time on learning key concepts.  If you came to this with your own priors, hopefully you'll have seen enough of the language and have a sense of how to proceed.  

Best of luck with your research!

<!--chapter:end:1000_Conclusion.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`



The following references will help you get going with Bayesian data analysis and Stan specifically.

Gelman et al. ***Bayesian Data Analysis***.  Gelman does not do the programming for Stan but has been one of the core members driving its development since day one.  This highly cited resource is something you can continue to go back to time and again, and has some detailed examples and discussion of HMC. (advanced, but highly conceptual and practical at the same)

McElreath, R. ***Statistical Rethinking***.  A good modeling book in general, by one who has contributed a lot to helping others learn Stan.  Comes with its own R package too. (intro to moderate)

Kruschke, J. ***Doing Bayesian Data Analysis***. Very intro book, but might be good for those not too confident in statistics generally speaking. And who doesn't like puppies? Second edition has Stan examples. (intro)

Me. [Bayesian Basics](http://m-clark.github.io/docs/IntroBayes.html). Concept-focused, and simple regression model to introduce Stan, with a little more by-hand stuff in the appendix. (intro)


http://mc-stan.org/ Main website

http://stan.fit/ The Stan Group

More resources [here](http://mc-stan.org/documentation/)

<!--chapter:end:1001_references.Rmd-->

